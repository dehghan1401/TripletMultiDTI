# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-cAxzSvU2QS4o9kqw5yfyS7cYv-XZg_L
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
"""
# %cd '/content/drive/My Drive/Project/Project/DeepDTI/'
!dir
!git clone https://github.com/Deshan-Zhou/MultiDTI.git
"""

! pip install networkx

import os
from collections import defaultdict
import math
import networkx as nx
import random
from tqdm import tqdm
from zipfile import ZipFile
from urllib.request import urlretrieve
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pandas as pd

# Read Data


# Build drug_drug graph
drug_drug_graph = nx.Graph()

data = pd.read_csv('/content/drive/My Drive/Project/Project/DeepDTI/MultiDTI/data/drug_drug.csv')

for i in range(1,709):
  col=data[data.columns[i]]
  col=np.array(col)
  idx=np.argwhere(col)
  if idx.shape[0] != 0:
    #print(idx.shape[0])
    for j in range(0,idx.shape[0]):
      drug_drug_graph.add_edge(i-1, idx[j,0], weight=1)
  else:
    col=data[data.columns[i]]
    col=np.array(col)
    idx=np.argwhere(col==0)
    #print(idx.shape[0])
    for j in range(0,idx.shape[0]):
      drug_drug_graph.add_edge(i-1, idx[j,0], weight=0.0001)

# Build protein_protein graph
protein_protein_graph = nx.Graph()

data = pd.read_csv('/content/drive/My Drive/Project/Project/DeepDTI/MultiDTI/data/protein_protein.csv')

for i in range(1,1513):
  col=data[data.columns[i]]
  col=np.array(col)
  idx=np.argwhere(col==1)
  if idx.shape[0] != 0:
    col=data[data.columns[i]]
    col=np.array(col)
    idx=np.argwhere(col==1)
    #print(idx.shape[0])
    for j in range(0,idx.shape[0]):
      protein_protein_graph.add_edge(i-1, idx[j,0], weight=1)
  else:
    col=data[data.columns[i]]
    col=np.array(col)
    idx=np.argwhere(col==0)
    #print(idx.shape[0])
    for j in range(0,idx.shape[0]):
      protein_protein_graph.add_edge(i-1, idx[j,0], weight=0.0001)




## Read drug smiles


CHARPROTSET = { "A": 1, "C": 2, "B": 3, "E": 4, "D": 5, "G": 6, 
				"F": 7, "I": 8, "H": 9, "K": 10, "M": 11, "L": 12, 
				"O": 13, "N": 14, "Q": 15, "P": 16, "S": 17, "R": 18, 
				"U": 19, "T": 20, "W": 21, 
				"V": 22, "Y": 23, "X": 24, 
				"Z": 25 }

CHARPROTLEN = 25

CHARCANSMISET = { "#": 1, "%": 2, ")": 3, "(": 4, "+": 5, "-": 6, 
			 ".": 7, "1": 8, "0": 9, "3": 10, "2": 11, "5": 12, 
			 "4": 13, "7": 14, "6": 15, "9": 16, "8": 17, "=": 18, 
			 "A": 19, "C": 20, "B": 21, "E": 22, "D": 23, "G": 24,
			 "F": 25, "I": 26, "H": 27, "K": 28, "M": 29, "L": 30, 
			 "O": 31, "N": 32, "P": 33, "S": 34, "R": 35, "U": 36, 
			 "T": 37, "W": 38, "V": 39, "Y": 40, "[": 41, "Z": 42, 
			 "]": 43, "_": 44, "a": 45, "c": 46, "b": 47, "e": 48, 
			 "d": 49, "g": 50, "f": 51, "i": 52, "h": 53, "m": 54, 
			 "l": 55, "o": 56, "n": 57, "s": 58, "r": 59, "u": 60,
			 "t": 61, "y": 62, "@": 63, "/": 64, "\\": 0}

protein_dict_len=65

def label_smiles(line, MAX_SMI_LEN, smi_ch_ind):
	X = np.zeros(MAX_SMI_LEN)
	for i, ch in enumerate(line[:MAX_SMI_LEN]): #	x, smi_ch_ind, y
		X[i] = smi_ch_ind[ch]

	return X #.tolist()

def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):
	X = np.zeros(MAX_SEQ_LEN)

	for i, ch in enumerate(line[:MAX_SEQ_LEN]):
		X[i] = smi_ch_ind[ch]

	return X #.tolist()


smiles_dict_len = 64

protein_max_len=1000
smiles_max_len=100


smiles_samples = pd.read_csv('/content/drive/My Drive/Project/Project/DeepDTI/MultiDTI/data/durg_smiles.csv',header=None)
smiles_rep=[]
for a in smiles_samples[1]:
   smiles_rep.append(label_smiles(a,smiles_max_len,CHARCANSMISET))


protein_samples = pd.read_csv('/content/drive/My Drive/Project/Project/DeepDTI/MultiDTI/data/protein_fasta.csv',header=None)
protein_rep=[]
for a in protein_samples[1]:
   protein_rep.append(label_sequence(a,protein_max_len,CHARPROTSET))

drug_protein_interaction = pd.read_csv('/content/drive/My Drive/Project/Project/DeepDTI/MultiDTI/data/drug_protein.csv',header=0)
print(drug_protein_interaction)
print(drug_protein_interaction.iloc[0,0])

count_p=0
count_n=0

positive_samples_d=[]
negative_samples_d=[]

positive_samples_p=[]
negative_samples_p=[]
for i in range(0,708):
  for j in range(1,1513):
    if drug_protein_interaction.iloc[i,j]==1:
       count_p=count_p+1
       positive_samples_d.append([i])
       positive_samples_p.append([j-1])
    else:
      count_n=count_n+1
      negative_samples_d.append([i])
      negative_samples_p.append([j-1])
print(count_p)
print(count_n)

idx = np.random.choice(count_n, count_p*10, replace=False)  
idx=np.reshape(idx,(-1,1))

positive_samples_d_orig=np.array(positive_samples_d)
negative_samples_d_orig=np.reshape(np.array(negative_samples_d)[idx],(-1,1))
positive_samples_p_orig=np.array(positive_samples_p)
negative_samples_p_orig=np.reshape(np.array(negative_samples_p)[idx],(-1,1))

import scipy
aa=np.reshape(np.tile(positive_samples_d_orig,10),(19230,1))
bb=np.reshape(np.tile(positive_samples_p_orig,10),(19230,1))
c=list(zip(aa,bb))
random.shuffle(c)
a, b = zip(*c)
positive_samples_d_orig=np.reshape(a,(19230,1))
positive_samples_p_orig=np.reshape(b,(19230,1))

degrees = []
for node in drug_drug_graph.nodes:
    degrees.append(protein_protein_graph.degree[node])
print("Average node degree:", round(sum(degrees) / len(degrees), 2))

# node2vec for Drug-Drug and Protein-Protein

vocabulary_drug = ["NA"] + list(drug_drug_graph.nodes)
vocabulary_lookup_drug = {token: idx for idx, token in enumerate(vocabulary_drug)}

vocabulary_protein = ["NA"] + list(protein_protein_graph.nodes)
vocabulary_lookup_protein = {token: idx for idx, token in enumerate(vocabulary_protein)}


def next_step(graph, previous, current, p, q):
    neighbors = list(graph.neighbors(current))

    weights = []
    # Adjust the weights of the edges to the neighbors with respect to p and q.
    for neighbor in neighbors:
        if neighbor == previous:
            # Control the probability to return to the previous node.
            weights.append(graph[current][neighbor]["weight"] / p)
        elif graph.has_edge(neighbor, previous):
            # The probability of visiting a local node.
            weights.append(graph[current][neighbor]["weight"])
        else:
            # Control the probability to move forward.
            weights.append(graph[current][neighbor]["weight"] / q)

    # Compute the probabilities of visiting each neighbor.
    weight_sum = sum(weights)
    probabilities = [weight / weight_sum for weight in weights]
    # Probabilistically select a neighbor to visit.
    next = np.random.choice(neighbors, size=1, p=probabilities)[0]
    return next


def random_walk(graph, num_walks,vocabulary_lookup, num_steps, p, q):
    walks = []
    nodes = list(graph.nodes())
    # Perform multiple iterations of the random walk.
    for walk_iteration in range(num_walks):
        random.shuffle(nodes)

        for node in tqdm(
            nodes,
            position=0,
            leave=True,
            desc=f"Random walks iteration {walk_iteration + 1} of {num_walks}",
        ):
            # Start the walk with a random node from the graph.
            walk = [node]
            # Randomly walk for num_steps.
            while len(walk) < num_steps:
                current = walk[-1]
                previous = walk[-2] if len(walk) > 1 else None
                # Compute the next node to visit.
                next = next_step(graph, previous, current, p, q)
                walk.append(next)
            # Replace node ids (movie ids) in the walk with token ids.
            walk = [vocabulary_lookup[token] for token in walk]
            # Add the walk to the generated sequence.
            walks.append(walk)

    return walks

##############################################

# Random walk return parameter.
p = 1
# Random walk in-out parameter.
q = 1
# Number of iterations of random walks.
num_walks = 5
# Number of steps of each random walk.
num_steps = 10


walks_drug = random_walk(drug_drug_graph, num_walks,vocabulary_lookup_drug, num_steps, p, q)
walks_protein = random_walk(protein_protein_graph, num_walks,vocabulary_lookup_protein, num_steps, p, q)

print("Number of drug walks generated:", len(walks_drug))
print("Number of protein walks generated:", len(walks_protein))

#################################################

def generate_examples(sequences, window_size, num_negative_samples, vocabulary_size):
    example_weights = defaultdict(int)
    # Iterate over all sequences (walks).
    for sequence in tqdm(
        sequences,
        position=0,
        leave=True,
        desc=f"Generating postive and negative examples",
    ):
        # Generate positive and negative skip-gram pairs for a sequence (walk).
        pairs, labels = keras.preprocessing.sequence.skipgrams(
            sequence,
            vocabulary_size=vocabulary_size,
            window_size=window_size,
            negative_samples=num_negative_samples,
        )
        for idx in range(len(pairs)):
            pair = pairs[idx]
            label = labels[idx]
            target, context = min(pair[0], pair[1]), max(pair[0], pair[1])
            if target == context:
                continue
            entry = (target, context, label)
            example_weights[entry] += 1

    targets, contexts, labels, weights = [], [], [], []
    for entry in example_weights:
        weight = example_weights[entry]
        target, context, label = entry
        targets.append(target)
        contexts.append(context)
        labels.append(label)
        weights.append(weight)

    return np.array(targets), np.array(contexts), np.array(labels), np.array(weights)


num_negative_samples = 10#4

# generate examples for Drug-Drug Network

targets_d, contexts_d, labels_d, weights_d = generate_examples(
    sequences=walks_drug,
    window_size=num_steps,
    num_negative_samples=num_negative_samples,
    vocabulary_size=len(vocabulary_drug),
)

# generate examples for Protein-Protein Network

targets_p, contexts_p, labels_p, weights_p = generate_examples(
    sequences=walks_protein,
    window_size=num_steps,
    num_negative_samples=num_negative_samples,
    vocabulary_size=len(vocabulary_protein),
)



batch_size = 1024


def create_dataset(targets, contexts, labels, weights, batch_size):
    inputs = {
        "target": targets,
        "context": contexts,
    }
    dataset = tf.data.Dataset.from_tensor_slices((inputs, labels, weights))
    dataset = dataset.shuffle(buffer_size=batch_size * 2)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    return dataset


dataset_drug = create_dataset(
    targets=targets_d,
    contexts=contexts_d,
    labels=labels_d,
    weights=weights_d,
    batch_size=batch_size,
)

dataset_protein = create_dataset(
    targets=targets_p,
    contexts=contexts_p,
    labels=labels_p,
    weights=weights_p,
    batch_size=batch_size,
)


learning_rate = 0.001



def create_model(vocabulary_size, embedding_dim):

    inputs = {
        "target": layers.Input(name="target", shape=(), dtype="int32"),
        "context": layers.Input(name="context", shape=(), dtype="int32"),
    }
    # Initialize item embeddings.
    embed_item = layers.Embedding(
        input_dim=vocabulary_size,
        output_dim=embedding_dim,
        embeddings_initializer="he_normal",
        embeddings_regularizer=keras.regularizers.l2(1e-6),
        name="item_embeddings",
    )
    # Lookup embeddings for target.
    target_embeddings = embed_item(inputs["target"])
    # Lookup embeddings for context.
    context_embeddings = embed_item(inputs["context"])
    # Compute dot similarity between target and context embeddings.
    logits = layers.Dot(axes=1, normalize=False, name="dot_similarity")(
        [target_embeddings, context_embeddings]
    )
    # Create the model.
    model = keras.Model(inputs=inputs, outputs=logits)
    return model


drug_embeding_dim=100
protein_embeding_dim=200

model_drug = create_model(len(vocabulary_drug), drug_embeding_dim)
model_drug.compile(
    optimizer=keras.optimizers.Adam(learning_rate),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
)

model_protein = create_model(len(vocabulary_protein), protein_embeding_dim)
model_protein.compile(
    optimizer=keras.optimizers.Adam(learning_rate),
    loss=keras.losses.BinaryCrossentropy(from_logits=True),
)

# 
history_drug = model_drug.fit(dataset_drug, epochs=100)
history_protein = model_protein.fit(dataset_protein, epochs=100)
drug_embeddings = model_drug.get_layer("item_embeddings").get_weights()[0]
print("Drug Embeddings shape:", drug_embeddings.shape)
protein_embeddings = model_protein.get_layer("item_embeddings").get_weights()[0]
print("Protein Embeddings shape:", protein_embeddings.shape)

###   Define model architecture for drug and protein
import tensorflow
import keras
from keras.models import Model
from keras.preprocessing import sequence
from keras.models import Sequential, load_model
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D
from keras.layers import Conv2D, GRU
from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Masking, RepeatVector, merge, Flatten, Concatenate
from keras.models import Model
from keras.layers import Bidirectional
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras import optimizers, layers
from tensorflow.keras import regularizers

embedding_size=256
num_filters=64
protein_filter_lengths=8
smiles_filter_lengths=4

Drug_input = Input(shape=(smiles_max_len,), dtype='int32',name='drug_input') 
Protein_input = Input(shape=(protein_max_len,), dtype='int32',name='protein_input')

Drug_Drug_input = Input(shape=(drug_embeding_dim,),name='drug_drug_input') 
Protein_Protein_input = Input(shape=(protein_embeding_dim,),name='protein_protein_input')

encode_smiles = Embedding(input_dim=smiles_dict_len+1, output_dim = embedding_size, input_length=smiles_max_len,name='smiles_embedding')(Drug_input) 
encode_smiles = Conv1D(filters=num_filters, kernel_size=smiles_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv1_smiles')(encode_smiles)
# encode_smiles = MaxPooling1D(2)(encode_smiles)  # test it is effectuve or not
encode_smiles = Conv1D(filters=num_filters*2, kernel_size=smiles_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv2_smiles')(encode_smiles)
#encode_smiles = MaxPooling1D(2)(encode_smiles)
encode_smiles = Conv1D(filters=num_filters*3, kernel_size=smiles_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv3_smiles')(encode_smiles)
#encode_smiles = MaxPooling1D(2)(encode_smiles)

    
encode_protein = Embedding(input_dim=protein_dict_len+1, output_dim = embedding_size, input_length=protein_max_len, name='protein_embedding')(Protein_input)
encode_protein = Conv1D(filters=num_filters, kernel_size=protein_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv1_prot')(encode_protein)
#encode_protein = MaxPooling1D(2)(encode_protein)
encode_protein = Conv1D(filters=num_filters*2, kernel_size=protein_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv2_prot')(encode_protein)
#encode_protein = MaxPooling1D(2)(encode_protein)
encode_protein = Conv1D(filters=num_filters*3, kernel_size=protein_filter_lengths,  activation='relu', padding='valid',  strides=1, name='conv3_prot')(encode_protein)
#encode_protein = MaxPooling1D(2)(encode_protein)
    
encode_protein = GlobalMaxPooling1D()(encode_protein)
encode_smiles = GlobalMaxPooling1D()(encode_smiles)

encode_interaction =Concatenate()([encode_protein,encode_smiles])
encode_interaction2 =Concatenate()([Protein_Protein_input,Drug_Drug_input])
#encode_interaction =Concatenate()([encode_interaction,encode_interaction2])

# Fully connected 
FC1 = Dense(1024, activation='relu', name='dense1')(encode_interaction)
FC2 = Dropout(0.5)(FC1)
FC2 = Dense(1024, activation='relu', name='dense2')(FC2)
FC2 = Dropout(0.5)(FC2)
FC2 = Dense(512, activation='relu', name='dense3')(FC2) #'dense3'

FC1_2 = Dense(1024, activation='relu', name='dense1_')(encode_interaction2)
FC2_2 = Dropout(0.5)(FC1_2)
FC2_2 = Dense(1024, activation='relu', name='dense2_')(FC2_2)
FC2_2 = Dropout(0.5)(FC2_2)
FC2_2 = Dense(512, activation='relu', name='dense3_')(FC2_2)

FC2 =Concatenate(name='output1')([FC2,FC2_2])  

FC3 = Dense(512, activation='relu', name='dense3_p')(FC2)
#FC2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1),name='output1')(FC2)
# And add a logistic regression on top
predictions = Dense(1, activation='sigmoid', name='dense4')(FC3) # if you want train model for active/inactive set activation='sigmoid'
#,activity_regularizer=regularizers.l2(1e-5)
embedding_model = Model(inputs=[Drug_input, Protein_input,Drug_Drug_input,Protein_Protein_input], outputs=[FC2])
model = Model(inputs=[Drug_input, Protein_input,Drug_Drug_input,Protein_Protein_input], outputs=[predictions])
full_model1 = Model(inputs=[Drug_input, Protein_input,Drug_Drug_input,Protein_Protein_input], outputs=[FC2, predictions])
full_model2 = Model(inputs=[Drug_input, Protein_input,Drug_Drug_input,Protein_Protein_input], outputs=[ predictions])
print(embedding_model.summary())
print(model.summary())

positive_samples_d=np.array(positive_samples_d)
positive_samples_p=np.array(positive_samples_p)

negative_samples_d=np.array(negative_samples_d)
negative_samples_p=np.array(negative_samples_p)

smiles_rep=np.array(smiles_rep)
protein_rep=np.array(protein_rep)

def generate_data_(batch_size):
    while True:
        input1 = []
        input2 = []
        input3 = []
        input4 = []

        output1 = []
        batch_counter=0
        i_c=0
        pindex = np.random.choice(count_p, int(batch_size/2), replace=True)  
        nindex = np.random.choice(count_n, int(batch_size/2), replace=False) 
        input1.append(np.concatenate((np.squeeze(smiles_rep[positive_samples_d[pindex,0]]),np.squeeze(smiles_rep[negative_samples_d[nindex,0]])),axis=0))
        input2.append(np.concatenate((np.squeeze(protein_rep[positive_samples_p[pindex,0]]),np.squeeze(protein_rep[negative_samples_p[nindex,0]])),axis=0))

        input3.append(np.concatenate((np.squeeze(drug_embeddings[positive_samples_d[pindex,0]]),np.squeeze(drug_embeddings[negative_samples_d[nindex,0]])),axis=0))
        input4.append(np.concatenate((np.squeeze(protein_embeddings[positive_samples_p[pindex,0]]),np.squeeze(protein_embeddings[negative_samples_p[nindex,0]])),axis=0))

        output1.append(np.concatenate((np.ones((int(batch_size/2),1)),np.zeros((int(batch_size/2),1))),axis=0))

        input1=np.squeeze(np.array(input1))
        input2=np.squeeze(np.array(input2))
        input3=np.squeeze(np.array(input3))
        input4=np.squeeze(np.array(input4))
        output1=np.squeeze(np.array(output1))
        yield [np.array(input1),np.array(input2),np.array(input3),np.array(input4)], np.array(output1)


METRICS = [
      #keras.metrics.TruePositives(name='tp'),
      #keras.metrics.FalsePositives(name='fp'),
      #keras.metrics.TrueNegatives(name='tn'),
      #keras.metrics.FalseNegatives(name='fn'), 
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR') # precision-recall curve
]
#adam=Adam(lr=0.001)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[METRICS]) 
es = EarlyStopping(monitor='prc', mode='max', verbose=1, patience=15)




#model.fit_generator(generate_data_(1000),steps_per_epoch = count_p/1000,epochs=10,callbacks=[es])

# Define Embedding Model
import numpy as np
positive_samples_d=np.array(positive_samples_d)
positive_samples_p=np.array(positive_samples_p)

negative_samples_d=np.array(negative_samples_d)
negative_samples_p=np.array(negative_samples_p)

smiles_rep=np.array(smiles_rep)
protein_rep=np.array(protein_rep)

margin_=0.2

# define the triplet semi hard loss
def TripletSemiHardLoss(y_true, y_pred):
    n_samples=tf.shape(y_pred)[0]
    anchor=y_pred[0:4:n_samples-3]
    positive=y_pred[1:4:n_samples-2]
    negative=y_pred[2:4:n_samples-1]
    dist_anchor_nagative = tf.norm(anchor-negative,axis=1)
    dist_anchor_positive = tf.norm(anchor-positive,axis=1)
    loss = tf.math.reduce_mean(tf.maximum( dist_anchor_positive - dist_anchor_nagative + margin_, 0.0))
    return loss


# provides batches of data to learn the model
def generate_triplet_data(batch_size,positive_samples_d,negative_samples_d,positive_samples_p,negative_samples_p,flag):
  while True:
        input1 = []
        input2 = []
        input3 = []
        input4 = []

        output1 = []
        output2 = []
        batch_counter=0
        i_c=0

        count_p=positive_samples_d.shape[0]
        count_n=negative_samples_d.shape[0]

        pindex = np.random.choice(count_p, int(batch_size/2), replace=False)  
        anchor_positive_pair = np.random.choice(pindex, (int(batch_size/4),2), replace=False)


        nindex = np.random.choice(count_n, int(batch_size/2), replace=False) 
         
        desc_negative=np.concatenate((np.squeeze(drug_embeddings[negative_samples_d[nindex,0]]),
                                      np.squeeze(protein_embeddings[negative_samples_d[nindex,0]])),axis=1)
        
        
        
        out=full_model1.predict([smiles_rep[negative_samples_d[nindex,0]],protein_rep[negative_samples_d[nindex,0]],
                            drug_embeddings[negative_samples_d[nindex,0]], protein_embeddings[negative_samples_d[nindex,0]]])
        desc_negative=out[0]
        
        # select the semi hard negative examples 
        for i in range(0,int(batch_size/4)):
          """desc_anchor=np.concatenate((np.squeeze(drug_embeddings[positive_samples_d[anchor_positive_pair[i,0],0]]),
                                      np.squeeze(protein_embeddings[positive_samples_p[anchor_positive_pair[i,0],0]])),axis=-1)
          desc_positive=np.concatenate((np.squeeze(drug_embeddings[positive_samples_d[anchor_positive_pair[i,1],0]]),
                                      np.squeeze(protein_embeddings[positive_samples_p[anchor_positive_pair[i,1],0]])),axis=-1)"""
          
          out=full_model1.predict([np.reshape(smiles_rep[positive_samples_d[anchor_positive_pair[i,0],0]],(1,-1)),
                                  np.reshape(protein_rep[positive_samples_d[anchor_positive_pair[i,0],0]],(1,-1)),
                            np.reshape(drug_embeddings[positive_samples_d[anchor_positive_pair[i,0],0]],(1,-1)), 
                            np.reshape(protein_embeddings[positive_samples_d[anchor_positive_pair[i,0],0]],(1,-1))])
          desc_anchor=out[0]

          out=full_model1.predict([np.reshape(smiles_rep[positive_samples_d[anchor_positive_pair[i,1],0]],(1,-1)),
                                  np.reshape(protein_rep[positive_samples_d[anchor_positive_pair[i,1],0]],(1,-1)),
                            np.reshape(drug_embeddings[positive_samples_d[anchor_positive_pair[i,1],0]],(1,-1)), 
                            np.reshape(protein_embeddings[positive_samples_d[anchor_positive_pair[i,1],0]],(1,-1))])
          desc_positive=out[0]

          #print(desc_positive)
          
          dist_anchor_nagative = np.linalg.norm(desc_anchor-desc_negative,axis=1)
          dist_anchor_positive = np.linalg.norm(desc_anchor-desc_positive)

          


          negative_selection_measure = dist_anchor_positive-dist_anchor_nagative#+margin_
          negative_arg_sorted = np.argsort(negative_selection_measure)[::-1]
  

          negative_idx = negative_arg_sorted[np.random.choice(1,1)]#np.random.choice(3,1)
          negative_idx2 = negative_arg_sorted[np.random.choice(5,1)]



          input1.append(np.concatenate((np.expand_dims(smiles_rep[positive_samples_d[anchor_positive_pair[i,0],0]],axis=0),
                                        np.expand_dims(smiles_rep[positive_samples_d[anchor_positive_pair[i,1],0]],axis=0),
                                        smiles_rep[negative_samples_d[negative_idx,0]],
                                        smiles_rep[negative_samples_d[negative_idx2,0]]),axis=-2))
          input2.append(np.concatenate((np.expand_dims(protein_rep[positive_samples_p[anchor_positive_pair[i,0],0]],axis=0),
                                        np.expand_dims(protein_rep[positive_samples_p[anchor_positive_pair[i,1],0]],axis=0),
                                        protein_rep[negative_samples_p[negative_idx,0]],
                                        protein_rep[negative_samples_p[negative_idx2,0]]),axis=-2))

          input3.append(np.concatenate((np.expand_dims(drug_embeddings[positive_samples_d[anchor_positive_pair[i,0],0]],axis=0),
                                        np.expand_dims(drug_embeddings[positive_samples_d[anchor_positive_pair[i,1],0]],axis=0),
                                        drug_embeddings[negative_samples_d[negative_idx,0]],
                                        drug_embeddings[negative_samples_d[negative_idx2,0]]),axis=-2))
          input4.append(np.concatenate((np.expand_dims(protein_embeddings[positive_samples_p[anchor_positive_pair[i,0],0]],axis=0),
                                        np.expand_dims(protein_embeddings[positive_samples_p[anchor_positive_pair[i,1],0]],axis=0),
                                        protein_embeddings[negative_samples_p[negative_idx,0]],
                                        protein_embeddings[negative_samples_p[negative_idx2,0]]),axis=-2))

          output1.append(np.concatenate((np.zeros((1,512)),
                                         np.zeros((1,512)),
                                         np.zeros((1,512)),
                                         np.zeros((1,512))),axis=-2))
          output2.append(np.concatenate((np.ones((1,1)),
                                         np.ones((1,1)),
                                         np.zeros((1,1)),
                                         np.zeros((1,1))),axis=-2))

        input1=np.reshape(np.array(input1),(int(batch_size/4)*4,-1))
        input2=np.reshape(np.array(input2),(int(batch_size/4)*4,-1))
        input3=np.reshape(np.array(input3),(int(batch_size/4)*4,-1))
        input4=np.reshape(np.array(input4),(int(batch_size/4)*4,-1))
        output1=np.array(output1)
        output2=np.array(output2)#np.squeeze()
        output1=np.reshape(output1,(int(batch_size/4)*4,512))
        output2=np.reshape(output2,(int(batch_size/4)*4,1))

        if flag==1:
          yield [np.array(input1),np.array(input2),np.array(input3),np.array(input4)], np.array(output1)
        else:
          yield [np.array(input1),np.array(input2),np.array(input3),np.array(input4)], np.array(output2)


full_model1.compile(optimizer=tf.optimizers.Adam(0.001), loss=[TripletSemiHardLoss]) 
full_model2.compile(optimizer=tf.optimizers.Adam(0.001), loss=['binary_crossentropy'], metrics=[METRICS]) 

es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=15)



from sklearn.model_selection import KFold
kf = KFold(n_splits=10)
kf.get_n_splits(smiles_rep)

count1=0
for train_index_p, test_index_p in kf.split(positive_samples_d_orig):
    count2=0
    for train_index_n, test_index_n in kf.split(negative_samples_d_orig):
      if count1==count2:
         positive_samples_d, positive_samples_d_test = positive_samples_d_orig[train_index_p], positive_samples_d_orig[test_index_p]
         negative_samples_d, negative_samples_d_test = negative_samples_d_orig[train_index_n], negative_samples_d_orig[test_index_n]
         positive_samples_p, positive_samples_p_test = positive_samples_p_orig[train_index_p], positive_samples_p_orig[test_index_p]
         negative_samples_p, negative_samples_p_test = negative_samples_p_orig[train_index_n], negative_samples_p_orig[test_index_n]
         full_model1.fit_generator(generate_triplet_data(2000,positive_samples_d,negative_samples_d,positive_samples_p,negative_samples_p,1),
                               steps_per_epoch = 10,epochs=5,callbacks=[es],
                               validation_data=generate_triplet_data(100,positive_samples_d_test,negative_samples_d_test,positive_samples_p_test,negative_samples_p_test,1),validation_steps=2)
         full_model2.fit_generator(generate_triplet_data(2000,positive_samples_d,negative_samples_d,positive_samples_p,negative_samples_p,2),
                               steps_per_epoch = 10,epochs=10,callbacks=[es],
                               validation_data=generate_triplet_data(100,positive_samples_d_test,negative_samples_d_test,positive_samples_p_test,negative_samples_p_test,2),validation_steps=2)
      count2=count2+1
    count1=count1+1